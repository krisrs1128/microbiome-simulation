# Microbiome Networks

Unlike human social networks, there is no simple way to observe microbe-microbe
interactions -- we have to make do with indirect evidence. One approach uses
population profiles as a proxy for ecological interaction. Taxa that often
co-occur are understood to have cooperative ecological interactions, while those
that don't are thought to compete for the same niche.

Many algorithms have been designed around this intuition, all trying to go
beyond simple co-occurrence and instead capture more complex types of
dependence. A challenge in practice is that it's hard to know which method to
use when, since the problem is unsupervised. Even when thorough simulation
benchmarking studies are available, it's often not obvious how well those
simulation setups match our problems of interest.

## Estimation

Let's use simulation to benchmark network estimation methods using data from
rounds 1 and 2 of the [American Gut
Project](https://journals.asm.org/doi/10.1128/msystems.00031-18). We will
simulate data with known correlation structure and taxa-level marginals
estimated from the study data. The block below reads in the data.

```{r, read-data}
counts <- read_csv("https://go.wisc.edu/kj2twy") |>
  column_to_rownames("taxon")
metadata <- read_csv("https://go.wisc.edu/gi9rb6")

exper <- SummarizedExperiment(
  assays = SimpleList(counts = as.matrix(counts)),
  colData = metadata
)
```

We've estimated a zero-inflated negative binomial location-shape-scale
(`ZINBLSS`) models for each taxon, using a gaussian copula to capture
dependence.  We have used the regression formula `~log(sequencing_depth) + BMI`.
The data structure below captures all the simulator components, and we can swap
pieces in and out to modify the form of the simulator. For example, if we
wanted, we could `mutate` the family and link function associated with
particular features.

```{r, example-sim}
sim <- setup_simulator(
  exper,
  ~ log(sequencing_depth) + BMI,
  ~ ZINBLSS()
) |>
  estimate(mstop = 100)
sim
```

The simulated data is always a `SummarizedExperiment`. This means that any
workflow that applied to the original data can be applied to the simulated one
without any changes. Notice also that `sample` defaults to drawing samples from
the same design as the original input experiment (we'll modify this using the
`new_data` argument in a minute).

```{r}
simulated <- sample(sim)
simulated
```

## Evaluation

Let's compare the marginal count distributions for the real and simulated data.
We'll need the data in "long" format to be able to make the ggplot2 figure. The
`pivot_experiment` helper can transform the original `SummarizedExperiment`
objects in this way. Notice that the simulated data tends to overestimate the
number of zeros in the high-abundance taxa. To refine the simulator, we should
probably replace the zero-inflated negative binomial with ordinary negative
binomials for these poorly fitted taxa.

```{r, histogram-critique, fig.width = 9, fig.height = 5}
bind_rows(
  real = pivot_experiment(exper),
  simulated = pivot_experiment(simulated),
  .id = "source"
) |>
  filter(feature %in% rownames(simulated)[1:20]) |>
  ggplot() +
  geom_histogram(
    aes(log(1 + value), fill = source),
    position = "identity", alpha = 0.8
  ) +
  facet_wrap(~ reorder(feature, value), scales = "free")
```

Are the learned relationships with BMI plausible? We can compare scatterplots of
the real and simulated data against this variable. Note that, by default, the
ribbons will be evaluated along all variables, which makes for the jagged
ribbons (neighboring values for BMI might have different sequencing depth,
potentially leading to quite different predictions). To remove this artifact, we
can assume that all samples had exactly the same sequencing depth.

```{r, bmi-scatter, fig.width = 12, fig.height = 4}
new_data <- colData(exper) |>
  as_tibble() |>
  mutate(sequencing_depth = 2e4)
plot(sim, "BMI", sample(sim, new_data = new_data), new_data = new_data)
```

We next visualize the correlation matrix estimated by the simulator's copula
model. There are a few pairs of taxa that are very highly correlated, and there
are also a few taxa that seem to have higher correlation across a large number
of taxa (e.g., the taxon in row 34). There is no obvious banding or block
structure in this real data, though.

```{r correlation-heatmap, fig.width = 5, fig.height = 5}
rho <- copula_parameters(sim)
heatmap(rho)
```

The pair below is one of those with high positive correlation. You can replace
the selection with the commented out line to see what one of the anticorrelated
pairs of taxa looks like.

```{r correlation_pairs, fig.width = 5, fig.height = 5}
#taxa <- rownames(exper)[c(33, 43)]
taxa <- rownames(exper)[c(14, 25)]
pivot_experiment(exper) |>
  filter(feature %in% taxa) |>
  tidyr::pivot_wider(names_from = feature) |>
  ggplot() +
  geom_point(aes(log(1 + .data[[taxa[1]]]), log(1 + .data[[taxa[2]]])))
```

# Block Covariance

Let's replace the current copula correlation structure with one from a block
diagonal matrix. In this example, the off-diagonal correlations are 0.6. We can
use `mutate_correlation` to swap this new correlation matrix into our earlier
simulator.

```{r mutate_correlation}
rho <- c(0.4, .6, 0.8) |>
  map(~ matrix(., nrow = 15, ncol = 15)) |>
  Matrix::bdiag() |>
  as.matrix()
diag(rho) <- 1

simulated <- sim |>
  mutate_correlation(rho) |>
  sample()

x <- t(assay(simulated))
```

Let's first look at the [`SpiecEasi` covariance estimate](https://github.com/zdk123/SpiecEasi). This is a variant of the
graphical lasso that is designed to be well-adapted to microbiome data. The good
news is that it does warn that the default choices of $\lambda$ are too large,
which is correct in this case. Unfortunately, it took a while to get this
answer, and we had already been quite generous in allowing it to fit 10 choices
of $\lambda$.

```{r spieceasi}
rho_se <- spiec.easi(x, nlambda = 10, pulsar.params = list(rep.num = 1)) |>  
  getOptCov() |>
  as.matrix() |>
  cov2cor()
heatmap(rho_se)
```

Let's instead use the Ledoit-Wolfe estimator on the log-transformed data. The
results make much more sense.

```{r lw}
rho_lw <- CovEst.2003LW(log(1 + x))$S |>
  cov2cor()
heatmap(rho_lw)
```

Since color comparisons are difficult to evaluate precisely, we can also make a
scatterplot comparing the different covariance estimators.

```{r covariance_metrics}
data.frame(truth = c(rho), se = c(rho_se), lw = c(rho_lw)) |>
  pivot_longer(-truth, values_to = "estimate") |>
  ggplot() +
  geom_jitter(aes(truth, estimate, col = name), alpha = 0.6, size = 0.4) +
  geom_abline(slope = 1) +
  facet_wrap(~name)
```

## Generalization

What about other network structures? We can use the same logic to evaluate
several network regimes. For example, the block below defines correlation
matrices associated with scale-free and banded structures.

```{r}
# will move this into MIGsim
library(rags2ridges)
N <- 1000
rho <- list(block = rho)
rho[["scale_free"]] <- createS(n = N, p = 45, topology = "scale-free", precision = TRUE) |>
  solve() |>
  cov2cor()
rho[["banded"]] <- createS(n = N, p = 45, topology = "banded", banded.n = 5, precision = TRUE) |>
  solve() |>
  cov2cor()

# dimnames(rho[["scale_free"]]) <- NULL
# dimnames(rho[["banded"]]) <- NULL
```

```{r}
rho_hat <- list()

for (r in seq_along(rho)) {
  x <- sim |>
    mutate_correlation(rho[[r]]) |>
    scDesigner::sample() |>
    assay() |>
    t()

  rho_se <- spiec.easi(x, nlambda = 10, pulsar.params = list(rep.num = 1)) |>  
    getOptCov() |>
    as.matrix() |>
    cov2cor()
  rho_lw <- CovEst.2003LW(log(1 + x))$S |>
    cov2cor()
  rho_hat[[names(rho)[r]]] <- list(se = rho_se, lw = rho_lw)
}
```

```{r}
# will move this into the MIGsim package
comparison <- data.frame(
  truth = map_dfr(rho, ~ c(.), .id = "source"),
  banded = map_dfr(rho_hat[["banded"]], ~ c(.), .id = "source"),
  block = map_dfr(rho_hat[["block"]], ~ c(.), .id = "source"),
  scale_free = map_dfr(rho_hat[["scale_free"]], ~ c(.), .id = "source")
)
colnames(comparison)[1:3] <- c("block.truth", "scale_free.truth", "banded.truth")

comparison <- comparison |>
  pivot_longer(everything()) |>
  separate(name, c("structure", "estimator"), sep = "\\.") |>
  group_by(estimator) |>
  arrange(structure) |>
  mutate(ix = row_number()) |>
  pivot_wider(names_from = "estimator") |>
  pivot_longer(se:lw, names_to = "method")

ggplot(comparison) +
  geom_jitter(aes(truth, value, col = method), width = 0.05, size = 0.8, alpha = 0.7) +
  geom_abline(slope = 1) +
  facet_wrap(~ structure, scales = "free")
```


This example shows that, when we start with real template data, it's not too
hard to setup a benchmarking experiment. It's generally easier to reconfigure
the components of an existing simulator than it is to specify all the simulation
steps from scratch. There is the secondary bonus that the data tend to look
close to real data of interest, at least up to the deliberate transformations
needed to establish ground truth.

We could imagine extending this example to include different data properties
(sample sizes, variable block sizes and correlations, more general correlation
structure) and estimation strategies (alternative transformations or
estimators). Design changes could be implemented using `expand_colData`, changes
in the signal can be specified as above with `mutate_correlation`, and any
workflow can be used as long as it applies to a `SummarizedExperiment` object.
