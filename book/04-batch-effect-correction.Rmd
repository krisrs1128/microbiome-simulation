# Batch Effect Correction

Integration can be a subtle exercise. We need to balance our interest in seeing
similarities between datasets with the risk of making things seem more similar
than they really are. Simulation can help navigate this subtlety by letting us
see how integration methods would behave in situations where we know exactly how
the different datasets are related. This note will illustrate this perspective
by showing how simulation can help with both horizontal (across batches) and
vertical (across assays) integration. We'll have a brief interlude on the `map`
function in the `purrr`, which is helpful for concisely writing code that would
otherwise need for loops (e.g., over batches or assays).

As usual, let's load the libraries we'll need. Remember that instructions for
installing `MIGsim` and `scDesigner` are documented in the repository
[README](https://github.com/krisrs1128/intro-to-simulation/).

```{r, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, cache = FALSE)
set.seed(20240603)
```

## Anaerobic Digestion Data

This example is about simultaneously analyzing several batches in a dataset
about the efficiency of anaerobic digestion (AD) of organic matter. The
essential problem is that, in this study, the samples could not be collected
simultaneously. Small differences across separate runs could lead to systematic
differences in the resulting data, which can obfuscate the more interesting
between-group variation that the experiment was intended to uncover. For
example, in the AD dataset, the date of the sequencing run has a global effect
on measured community composition, which we can see right away from a principal
components plot:

```{r, pca-anaerobic}
data(anaerobic)
pca_batch(anaerobic, facet = FALSE) +
  scale_color_manual(values = c("#191C59", "#bc0c3c")) +
  labs(
    col = "Treatment",
    shape = "Batch",
    x = "PC1 [25.2% Variance Explained]",
    y = "PC2 [16.2% Varianced Explained]"
  )
```
```{r, echo = FALSE, eval = FALSE}
ggsave("~/Downloads/original_batch_effects.png", width = 5, height = 3)
```

You can learn more about the general microbiome batch effect integration problem
in [(Wang and Le Cao, 2020)](https://doi.org/10.1093/bib/bbz105), which is where
this dataset example and the batch effect correction code below comes from. The
article also reviews mechanisms that could lead to batch effects in microbiome
data, together with methods for removing these effects and the situations within
which they are most appropriate.

In batch effect correction, it's important to remove as much of the batch
variation as possible without accidentally also removing the real biological
variation that would have been present even if all the samples had been
sequenced together. This is sometimes called ``overintegration,'' and this is an
especially high risk if some of the real biological variation is quite subtle,
e.g., a rare cell type or one that is very similar to a more prominent one.
Simulation can help us gauge the extent to which different methods may or may
not overintegrate. Since we get to control the between-batch and and
between-biological-condition differences, we can see the extent to which
integration methods can remove the former while preserving the latter.

## Simulation Design

The block below estimates a candidate simulator. By using the formula `~ batch +
treatment`, we're allowing for taxon-wise differences due to batch and
treatment. Note that in principle, we could estimate an interaction between
batch and treatment (the treatment could appear stronger in some batches than
others). I encourage you to try estimating that model; however, visually
analyzing the output suggests that this full model has a tendancy to overfit.
Since the data have already been centered log-ratio transformed, we can try out
a Gaussian marginal model. The AD dataset has relatively few samples
compared to the number of features, so we'll use a copula that's designed for
this setting.

```{r, anaerobic-simulator}
simulator <- setup_simulator(
  anaerobic,
  ~ batch + treatment,
  ~ GaussianLSS(),
  copula = copula_adaptive(thr = .1)
) |>
  estimate(nu = 0.05, mstop = 100) # lower nu -> stable training
```

We can simulate from the fitted model and evaluate the quality of our fit using
`contrast_boxplot`. This is a light wrapper of the ggplot2 code we used to
compare experiments from our first session, and you can read its definition
[here](https://github.com/krisrs1128/intro-to-simulation/blob/10fa498aea952684204b2f15c387a7983c30626d/MIGsim/R/plot.R#L26).

```{r, eval-anaerobic}
anaerobic_sim <- sample(simulator)
contrast_boxplot(anaerobic, anaerobic_sim)
```

**Exercise** Propose and create at least one other visualization that can be
used to compare and contrast the simulator with real data. What conclusions can
you draw?

**Solution**: There are many possible answers:

* Boxplots across taxa with different overall abundance levels.
* Analogous histograms or CDF plots, to show the entire distributions, rather than just summarized quantiles.
* Pair scatterplots, to see how well the bivariate relationships between taxa are preserved.
* Dimensionality reduction on the simulated data, to see how well it matches global structure in the original data.

We'll implement the last idea using PCA. This should be contrasted with the PCA
plot on the original data above. It's okay if the plot seems rotated relative to
the oiginal plot -- PCA is only unique up to rotation. The main characteristic
we're looking for is that the relative sizes of the batch and treatment effects
seem reasonaly well-preserved, since these will be the types of effects that our
later batch effect integration methods must be able to distinguish.

```{r}
pca_batch(anaerobic_sim)
```
:::

To study the risk for overintegration, let's imagine that there were a third
treatment group with relatively fewer samples. This is the type of group that a
correction method might accidentally blend in with the rest, if it's too
aggressive. We've defined the imaginary experiment using the data.frame below.
The `treatment` level `1.8` is the new one. We've supposed there are between 1 -
3 technical replicates (`extraction`) for each biological sample (`sample`), and
the batch dates are the same as before.

```{r}
data(imaginary_design)
summary(imaginary_design)
```

We can simulate from the new design and look at how different this new treatment
group seems from the others. It's a subtle effect, definitely smaller than the
batch effect, but also separate enough that we should be able to preserve it.

```{r}
p <- list()
anaerobic_sim <- sample(simulator, new_data = imaginary_design)
p[["sim"]] <- pca_batch(anaerobic_sim)
```

## Benchmarking

We've defined a `batch_correct` wrapper function that implements either the
RUV-III or ComBat batch effect correction methods. Their outputs are contrasted
in the PCAs below. It looks like ComBat might be somewhat too aggressive,
causing the `1` and `1.8` treatment groups to substantially overlap, while RUV
is a bit more conservative, keeping the treatment groups nicely separate. As an
aside, we note that this conclusion can depend on the number of replicates and
total number of samples available. We've included the code for generating the
`imaginary_design` data.frame in [a vignette](https://github.com/krisrs1128/intro-to-simulation/blob/10fa498aea952684204b2f15c387a7983c30626d/MIGsim/vignettes/process-integration.Rmd#L76)
for the `MIGsim` package. Can you find settings that lead either method astray?

```{r}
p[["ruv"]] <- pca_batch(batch_correct(anaerobic_sim, "ruv"))
p[["combat"]] <- pca_batch(batch_correct(anaerobic_sim, "combat"))
```

```{r batch-correction-vis, echo = FALSE}
map_dfr(p, ~ .$data, .id = "source") |>
  mutate(
    source = forcats::fct_recode(source, ComBat = "combat", `RUV-III` = "ruv", Simulation = "sim"),
    source = factor(source, levels = c("Simulation", "RUV-III", "ComBat"))
    ) |>
  ggplot(aes(col = factor(treatment))) +
  geom_vline(xintercept =  0) +
  geom_hline(yintercept =  0) +
  geom_point(aes(PC1, PC2)) +
  scale_color_manual(values = c("#191C59", "#f20732", "#04B2D9")) +
  labs(col = "Treatment") +
  facet_grid(source ~ reorder(batch, PC1))
ggsave("~/Downloads/batch_corrected.png", width = 8, height = 3, dpi=400)
```

```{r, evaluate-predictions}
prediction_errors <- function(df) {
  y_hat <- predict(lda(treatment ~ PC1 + PC2, data = df), type = "response")$class
  table(df$treatment, y_hat)
}

map(p, ~ prediction_errors(.$data))
```

```{r}
sessionInfo()
```