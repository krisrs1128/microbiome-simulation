# Multivariate Power Analysis

How can we choose sample sizes in more complex bioinformatic workflows, where we
simultaneously analyze many features (taxa, genes, metabolites) in concert?
While traditional, analytical power analysis often breaks down, simulation can
still be effective. We'll look at a concrete case study where we try to choose a
good sample size for a sparse partial least squares discriminant analysis
(sPLS-DA) of the Type I Diabetes (T1D) gut microbiome.

```{r, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE)
set.seed(202406)
library(tidyverse)
```

First, we'll load the required packages. Relative to our first session, the only
additional package that we need is `mixOmics`. This can be installed using
`BiocManager::install('mixOmics')`.

## Interpreting PLS-DA

The T1D dataset below describes 427 metabolites from the gut microbiomes of 40
T1D patients and 61 healthy controls. These data were originally gathered by
[Gavin et al. (2018)](https://doi.org/10.2337/dc18-0777), who were motivated by
the relationship between the pancreas and intestinal issues often experienced by
T1D patients. They were especially curious about whether microbime-associated
proteins might be related to increased risk for T1D development.

```{r load_t1d}
data(t1d)
```

The output that we care about are the PLD-DA scores and loadings. The wrapper
below directly gives us this output without our having to explicitly set
hyerparameters, though you can [look here](https://github.com/krisrs1128/intro-to-simulation/blob/main/MIGsim/R/methods.R) 
to see how the function was defined.

```{r initial_t1d_analysis}
result <- splsda_fit(t1d)
plotIndiv(result$fit, cex = 5)
plotVar(result$fit, cutoff = 0.4, cex = 5)
```

**Exercise**: Discuss the output of `plotIndiv`. How does `plotVar` shape
your interpretation?


**Solution**: The first two dimensions of the sPLS-DA pick on distinctions between the T1D
(orange) and control (blue) groups. The variance by these first two dimensions
is relatively small
-- variation in protein signatures may not be easiliy captured in two dimensions
or may not generally be correlated with the disease response. Since most of the
differences between groups are captured by the first dimension, we can look for
features that are highly correlated with this dimension to see which proteins
might be driving the difference. For example, we can conclude that T1D samples
tend to have higher levels of
[p21333](https://www.uniprot.org/uniprotkb/P21333/entry) and
[q8wz42](https://www.uniprot.org/uniprotkb/Q8WZ42/entry) compared to the
controls. 

## Estimation

Let's estimate a simulator where every protein is allowed ot vary across T1D
type. Since the data have already been centered log-ratio transformed, it's okay
to treat these as Gaussian. 

```{r initial_estimator_multivariate}
simulator <- setup_simulator(t1d, link_formula = ~outcome2, family = ~ GaussianLSS()) |>
  estimate(mstop = 100)
```

## Evaluation

Last time, we saw how we could visualize marginal simulator quality. How can we
tell whether a joint simulator is working, though? One simple check is to
analyze the pairwise correlations. Since the copula model is designed to capture
second-order moments, it should at the very least effectively capture the
correlations.

We've written a small helper that visualizes the pairwise protein-protein
correlations from both the real and the simulated datasets. We seem to be often
overestimating the correlation strength. This is likely a consequence of the
high-dimensionality of the problem.

```{r multivariate_correlations}
sim_exper <- sample(simulator)
correlation_hist(t1d, sim_exper)
```

**Exercise**: To address this, let's try modifying the `copula_def` argument of
`setup_simulator` to use a more suitable simulator. Generate new correlation
histograms and comment on the changes you observe. You only need to modify the
commented lines (`#`) lines in the block below.

```{r t1d_simulation_exercise, eval = FALSE}
simulator <- setup_simulator(
  t1d,
  link_formula = ~outcome2,
  family = ~ GaussianLSS(),
  copula_def = # fill this code in
  ) |>
  estimate(mstop = 100)

sim_exper <- sample(simulator) # and then run these two lines
correlation_hist(t1d, sim_exper) #
```

**Solution**: For our coupla, we can use a covariance estimator of [Cai and Liu
(2011)](https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10560), that is
suited for high dimensions. Larger values of `thr` will increase the stability
of our estimates, but at the cost of potentially missing or weakening true
correlations. In line with this point, our new simulated correlations are more
concentrated.

```{r estimate_t1d_simulator}
simulator <- setup_simulator(
  t1d,
  link_formula = ~outcome2,
  family = ~ GaussianLSS(),
  copula_def = copula_adaptive(thr = 0.1)
) |>
  estimate(mstop = 100)

sim_exper <- sample(simulator)
correlation_hist(t1d, sim_exper)
```
:::

```{r t1d_contrast_boxplot}
contrast_boxplot(t1d, sim_exper, ~. ~ reorder(outcome2, value))
```

```{r correlation_plot}
rhos <- list(sim = cor(t(assay(sim_exper))), real = cor(t(assay(t1d))))
diag(rhos[[1]]) <- NA
top_cors <- which(abs(rhos[[1]]) > quantile(abs(rhos[[1]]), 0.999, na.rm = TRUE), arr.ind = TRUE)

pairs_data <- list(
  real = subset_correlated(assay(t1d), top_cors),
  simulated = subset_correlated(assay(sim_exper), top_cors)
) |>
  bind_rows(.id = "source")

keep_pairs <- pairs_data |>
  dplyr::select(source, pair, correlation) |>
  unique() |>
  filter(source == "real", correlation > 0.72, correlation < 0.8) |>
  pull(pair)

p1 <- pairs_data |>
  filter(pair %in% keep_pairs) |>
  ggplot() +
  geom_point(aes(feature1, feature2, col = source), alpha = 0.5, size = 1) +
  scale_color_manual(values = c("#038C8C", "#D93636")) +
  guides(color = guide_legend(override.aes = list(alpha = 1, size = 2))) +
  labs(title = "A", col = "Source", x = expression("Feature" ~ j), y = expression("Feature" ~ j^"'")) +
  facet_wrap(~ reorder(pair, -correlation), ncol = 6, scales = "free") +
  theme(
    strip.text = element_text(size = 7),
    axis.text = element_text(size = 8)
  )
```

```{r collect_plots_multivariate}
ft_order <- hclust(dist(assay(t1d)))$order
p2 <- correlation_heatmap(rhos$real, ft_order) +
  labs(title = "B  Real") +
  (correlation_heatmap(rhos$sim, ft_order) +
    labs(title = "Simulated") +
    theme(legend.position = "none"))

(p1 / p2) +
  plot_layout(guides = "collect", heights = c(1.2, 2))
```

```{r save_multivariate, echo = FALSE}
ggsave("multivariate_evaluation_qualitative.svg", width = 10, height = 8)
```

How did we choose this threshold? You can compare the correlations more
systematically using this loop:

```{r thresholding_metrics}
bivariate_metrics <- \(thr, n_rep = 5) {
  # refit the simulator with current threshold
  simulator <- setup_simulator(
    t1d,
    link_formula = ~outcome2,
    family = ~ GaussianLSS(),
    copula_def = copula_adaptive(thr = thr)
  ) |>
    estimate(mstop = 100)

  # compute metrics on five samples
  err <- list()
  for (i in 1:n_rep) {
    sim_exper <- sample(simulator)[rownames(t1d), ]
    rhos <- list(sim = cor(t(assay(sim_exper))), real = cor(t(assay(t1d))))
    err[[i]] <- data.frame(
      frobenius = sqrt(mean((rhos$sim - rhos$real)^2)),
      ks = ks.test(rhos$sim, rhos$real)$statistic,
      thr = thr
    )
  }

  do.call(rbind, err)
}
```

```{r metrics_results}
errors <- seq(.001, 0.2, length.out = 10) |>
  map_dfr(bivariate_metrics) |>
  pivot_longer(-thr, names_to = "metric")

ggplot(errors) +
  geom_point(aes(thr, value)) +
  facet_wrap(~metric, scales = "free_y") +
  labs(x = "Threshold", y = "Metric Value")
```

## PLS-DA Power Analysis

Now that we have a simulator, we can run a power analysis. In theory, we could
look at how any summary from the PLS-DA output varies as the sample size
increases. The most natural one, though, is simply to see how classifier
performance improves as we gather more samples. Specifically, we'll measure the
holdout Area Under the Curve (auc), a measure of how well the trains PLS-DA
classifier balance precision and recall on new samples.

Moreover, we'll study the effect of sparsity -- what happens when many features
have no relationship at all with the response? We'll also simulate three
hypothetical datasets for each sample size and sparsity level. All
configurations of interest are stored in the `config` matrix below.

```{r power-analysis-params}
config <- expand.grid(
  sample_size = floor(seq(20, 200, length.out = 5)),
  n_rep = 1:20,
  n_null = floor(seq(317, 417, length.out = 4)),
  metrics = NA
)

data(t1d_order)
```

**Exercise**: Finally, we're in a position to generate synthetic data and
evaluate PLS-DA performance. Fill in the block below to update the simulator for
each `i`. Remember that the original `simulator` defined above assumes that all
proteins are associated with T1D. You can use `t1d_order` to prioritize the
proteins with the strongest effects in the original data. As before, you should
only need to modify the line marked with the comments (`#`).

```{r power-analysis-loop-setup, eval = FALSE}
for (i in seq_len(nrow(config))) {
  simulator <- simulator |>
    mutate(
      # fill this in
    ) |>
    estimate(mstop = 100)

  config$metrics[i] <- (sample_n(simulator, config$sample_size[i]) |>
    splsda_fit())[["auc"]]
  print(glue("run {i}/{nrow(config)}"))
}
```

**Solution**: To define nulls, we mutate the weakest proteins so that there is no longer any
association with T1D: `link = ~ 1` instead of `link = ~ outcome2`. To speed up
the computation, we organized the `mutate` calls so that we don't need to
re-estimate proteins whose effects were removed in a previous iteration.
```{r power-analysis-loop-soln, eval = FALSE}
for (i in seq_len(nrow(config))) {
  if (i == 1 || config$n_null[i] != config$n_null[i - 1]) {
    simulator <- simulator |>
      mutate(any_of(rev(t1d_order)[1:config$n_null[i]]), link = ~1) |>
      estimate(mstop = 100)
  }

  config$metrics[i] <- (sample_n(simulator, config$sample_size[i]) |>
    splsda_fit())[["auc"]]
  print(glue("run {i}/{nrow(config)}"))
}
```

```{r load_config}
config <- readRDS(url("https://uwmadison.box.com/shared/static/tf1xyo7a2n2rd2ms4y9itaev42dsnp5c.rds"))
```
:::

We can visualize variation in performance. 

```{r visualize-power, fig.width = 8, fig.height = 7.5}
p3 <- ggplot(config, aes(factor(sample_size), metrics, col = factor(n_null))) +
  stat_pointinterval(position = "dodge") +
  scale_color_scico_d(palette = "managua") +
  labs(x = "Sample Size", y = "AUC", color = "# Nulls", title = "C")

(p1 / p2 / p3) +
  plot_layout(guides = "collect", heights = c(1.2, 2, 1))
```

```{r save_multivariate_curve, echo = FALSE}
ggsave("multivariate_power_curve.svg", width = 10, height = 7.5, bg = "transparent")
```

**Discussion**: Interpret the visualization above. How do you think analysis
like this could help you justify making some experimental investments over
others?

**Solution**: Reading across facets from top left to bottom right, power decreases when the
number of null proteins increases. It seems that sPLS-DA can benefit from having
many weakly associated features. While power is sometimes high in low sample
sizes, the variance can be quite large. In all settings, there is a noticeable
decrease in variance in power as we go from 15 to 48 samples. If we can assume
that a moderate fraction (> 15\%) of measured proteins are associated with T1D,
then we may already achieve good power with ~ 100 samples. However, if we
imagine our effect might be sparser in our future experiment, then this figure
would give us good justification for arguing for a larger number of samples, in
order to ensure we can discover a disease-associated proteomics signature.

```{r}
ix <- colData(t1d)$outcome2 == "T1D"
p_vals <- apply(assay(t1d), 1, \(x) t.test(x[ix], x[-ix])$p.value)
storey_pi0_est(p_vals, 0.5)$pi0
```

## Comparison with $t$-test calculation

What if we had based our power analysis using theory from $t$-tests? We might be
hopeful that this theory might give a sample size that is also good enough for
the sPLS-DA. To make this concrete, suppose that we will test all 427 taxa using
a two-sample $t$-test with a Benjamini-Hochberg correction for multiple testing.
Suppose that we are in the sparse case, with only 10 nonnull taxa.  As a
hypothetical true signal strength, we use the observed signal strength of the
tenth most significant taxon from the real data,

```{r}
subset_size <- 10
y <- colData(t1d)$outcome2
x <- assay(t1d)[t1d_order[subset_size], ]
test_result <- t.test(x[y == "T1D"], x[y == "healthy"])
signal_strength <- test_result$statistic
signal_strength
```

How many samples would we need in order to achieve 80% power in this setting?
Accounting for multiple testing, we would need this taxon to be significant at
the level $\frac{10 \alpha}{427}$, which gives us,
```{r traditional_power}
result <- pwr.t.test(power = 0.8, d = signal_strength, sig.level = 0.05 * subset_size / nrow(t1d))
result$n
```

So, according to this power analysis, we only need 11 samples each for the
healthy and disease groups. In comparison to our earlier analysis using sPLS-DA,
this is very optimistic. In the case where only 10 taxa are significant, then
when we had 20 samples total, the AUC was in the range $\left[\right]$, which is
only slightly better than random guessing. This is a case where a $t$-test power
calculation would make us overoptimistic about the number of samples that are
required. Both the data generation (two shifted normal distributions) and
analysis ($t$-tests with BH correction) mechanisms assumed by this power
analysis are suspect, and in this case it makes a difference. All power analysis
requires some simplification, but the fact that a more realistic simulator
points us towards noticeably larger sample sizes is an indication that we should
not trust the $t$-test calculation and instead suggest the larger sample sample
from our sPLS-DA specific analysis.

```{r}
sessionInfo()
```
