# Simulating Differential Abundance

```{r, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE)
set.seed(202406)
```

Before we consider simulating entire microbial communities, with their complex
correlation structures, let's learn simulators for individual taxa. This is
already enough to analyze taxon-level differential abundance approaches.  For
example, at the end of this session, we'll apply a simulator to study the power
and false discovery rate of limma-voom when applied to microbiome data (as
opposed to the bulk RNA-seq data for which it was originally proposed).  Also,
marginal modeling is a first step towards multivariate (community-wide)
modeling, which we'll explore in the next session.

Let's load the necessary packages. Instructions for `scDesigner` and `MIGsim`
can be found in the pre-workshop announcement.  `SummarizedExperiment` is on
Bioconductor, and all other packages are on CRAN.

Let's train a simulator to fit the Atlas dataset. We'll use two
covariates. `bmi_group` is the main covariate of interest -- we want to see how
microbiome composition varies among people with different BMI. The `log_depth`
term is used to adjust for differential sequencing depths. We found it helpful
to fixed zero inflation across the population (`nu`), so we have set `nu = ~1`.
Finally, since we want to eventually evaluate testing methods that are designed
for count data, we have used the (Z)ero (I)nflated (N)egative (B)inomial
location-shape-scale model.

```{r atlas-estimation}
data(atlas)

fmla <- list(
  mu = ~ bmi_group + log_depth,
  sigma = ~ bmi_group + log_depth,
  nu = ~1
)
sim <- setup_simulator(atlas, fmla, ~ ZINBLSS()) |>
  estimate(nu = 0.01, mstop = 1000)
```

## Critique

**Exercise**: The block below combines the real and simulated experiments and
visualizes their difference. With your neighbors, discuss how well the simulator
approximates the original template.

```{r, contrast-simulators}
combined <- bind_rows(
  real = pivot_experiment(atlas), # real data
  simulated = pivot_experiment(sample(sim)), # simulated
  .id = "source"
)

ggplot(combined) +
  geom_boxplot(
    aes(log(1 + value), reorder(feature, value, median), fill = bmi_group)
  ) +
  facet_grid(. ~ source)
```

**Solution**: The clearest difference is that, for these more abundant taxa, there there are
not many low or zero counts. In contrast, the simulated data tend to have a long
left tail (the many outlier circles on the left side of the boxplots),
reflecting the fact that samples from the negative binomial distribution usually
have support for all counts $\geq 0$.  Nonetheless, the ordering of abundances
between the groups typically agrees between the real and simulated data. The
interquartile ranges for each taxon also seem to roughly match.

## Power Analysis Loop

To run a power analysis, we need to define datasets that have known ground
truth.  Then, we can run any differential abundance methods we want and see how
many of the true associations are recovered (and how many nulls are falsely
rejected). To this end, we'll remove associations from 16 of the original 24
genera, just like we removed group interactions in our spline fits above. We'll
choose to remove the 16 that have the weakest associations in the original data.
This is helpful because, even if we use `bmi_group` in our formula, if in
reality there is no (or very weak) effect, then even if our simulator considers
it as a true signal, the difference may be hard to detect. Eventually, our
package will include functions for modifying these effects directly; at this
point, though, we can only indirectly modify parameters by re-estimating them
with new formulas.

```{r, define-null}
nulls <- differential_analysis(atlas, "LIMMA_voom") |>
  rownames() |>
  tail(16)

null_fmla <- list(mu = ~log_depth, sigma = ~log_depth, nu = ~1)
sim <- sim |>
  mutate(any_of(nulls), link = null_fmla) |>
  estimate(nu = 0.01, mstop = 1000)
```

Now that we have ground truth associations, we'll evaluate LIMMA-voom for
differential analysis. We consider sample sizes ranging from 50 to 1200, and we
simulate 10 datasets for each sample size.

```{r, run-loop, eval = FALSE}
config <- expand.grid(
  sample_size = floor(seq(50, 1200, length.out = 5)),
  n_rep = 1:10
) |>
  mutate(run = as.character(row_number()))

results <- list()
for (i in seq_len(nrow(config))) {
  atlas_ <- sample_n(sim, config$sample_size[i])
  results[[i]] <- differential_analysis(atlas_, "LIMMA_voom") |>
    da_metrics(nulls, level = 0.3)
  print(glue("{i}/{nrow(config)}"))
}
```

**Exercise**: Visualize the results. How would you interpret the results of the
power analysis? Based on your earlier critique of the simulator, do you think
the estimated power here is conservative, liberal, or about right?

**Solution**: We'll use the `stat_pointinterval` function from the `ggdist` package to
visualize the range of empirical power estimates across sample sizes. We can see
that the average false discovery proportion is always controlled below 0.3,
though the variance in this proportion can be quite high. We can also see that
we would have quite good power with $n \geq 625$ samples, but the worst case
scenarios can be quite poor for anything with fewer samples.

```{r, visualize-da-power, eval = FALSE}
bind_rows(results, .id = "run") |>
  left_join(config) |>
  ggplot() +
  stat_pointinterval(aes(factor(sample_size), value)) +
  facet_wrap(~metric, scales = "free")
```

We expect that this result is somewhat conservative. This is because the
original data have more symmetric distributions than our simulation, so limma's
transformation to normality is likely easier to accomplish than in our more 
highly skewed data.


```{r}
sessionInfo()
```